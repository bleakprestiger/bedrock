{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir source_documents && cd source_documents && wget https://github.com/imartinez/privateGPT/blob/main/source_documents/state_of_the_union.txt","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:47:38.067731Z","iopub.execute_input":"2023-07-16T16:47:38.068154Z","iopub.status.idle":"2023-07-16T16:47:39.536244Z","shell.execute_reply.started":"2023-07-16T16:47:38.068119Z","shell.execute_reply":"2023-07-16T16:47:39.534648Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2023-07-16 16:47:39--  https://github.com/imartinez/privateGPT/blob/main/source_documents/state_of_the_union.txt\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 47334 (46K) [text/plain]\nSaving to: ‘state_of_the_union.txt’\n\nstate_of_the_union. 100%[===================>]  46.22K  --.-KB/s    in 0.01s   \n\n2023-07-16 16:47:39 (3.03 MB/s) - ‘state_of_the_union.txt’ saved [47334/47334]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir models && cd models && wget https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q4_0.bin","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:47:39.538894Z","iopub.execute_input":"2023-07-16T16:47:39.539281Z","iopub.status.idle":"2023-07-16T16:48:32.694964Z","shell.execute_reply.started":"2023-07-16T16:47:39.539243Z","shell.execute_reply":"2023-07-16T16:48:32.691413Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-07-16 16:47:40--  https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q4_0.bin\nResolving huggingface.co (huggingface.co)... 65.9.86.79, 65.9.86.57, 65.9.86.71, ...\nConnecting to huggingface.co (huggingface.co)|65.9.86.79|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/7e/26/7e26b3c7ced64f7024dbcce87fddd78a593c50c86955e5a756d14710387ada70/d1735b93e1dc503f1045ccd6c8bd73277b18ba892befd1dc29e9b9a7822ed998?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nous-hermes-13b.ggmlv3.q4_0.bin%3B+filename%3D%22nous-hermes-13b.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1689784362&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTc4NDM2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS8yNi83ZTI2YjNjN2NlZDY0ZjcwMjRkYmNjZTg3ZmRkZDc4YTU5M2M1MGM4Njk1NWU1YTc1NmQxNDcxMDM4N2FkYTcwL2QxNzM1YjkzZTFkYzUwM2YxMDQ1Y2NkNmM4YmQ3MzI3N2IxOGJhODkyYmVmZDFkYzI5ZTliOWE3ODIyZWQ5OTg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=X3YHrNeuxedgGj1oFsLBswpF%7EQ%7ExlybYkJVeby8Q1hQYR2KZEQzcpD0-qhIUQnm0c2NcXgBpfLodOvGbs4m8mJSybOV8ZST89LxECGPNsiEXXSjVQOf3pSyEE%7EwrZkWV0Td25yptJTVY8Y7PmopT7nzwKf7dn6vkHXC7pdL2PSt7Fv3jFYm1wjQQ5cUhWwPYi0j%7E2S1KCJjlb-VxF7iL4F92qP09Ld1Cn3fYnn4W-lctKMBs0DSr-pOK3IyNaQmJUj%7EE81hquErLEUETLzUPlOmMwRRUX%7EUBmfIAMpIVBF10tEieBt0drutOvV9EDJH1INEIOCb3tBl43IuXw8vQDg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2023-07-16 16:47:40--  https://cdn-lfs.huggingface.co/repos/7e/26/7e26b3c7ced64f7024dbcce87fddd78a593c50c86955e5a756d14710387ada70/d1735b93e1dc503f1045ccd6c8bd73277b18ba892befd1dc29e9b9a7822ed998?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nous-hermes-13b.ggmlv3.q4_0.bin%3B+filename%3D%22nous-hermes-13b.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1689784362&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTc4NDM2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS8yNi83ZTI2YjNjN2NlZDY0ZjcwMjRkYmNjZTg3ZmRkZDc4YTU5M2M1MGM4Njk1NWU1YTc1NmQxNDcxMDM4N2FkYTcwL2QxNzM1YjkzZTFkYzUwM2YxMDQ1Y2NkNmM4YmQ3MzI3N2IxOGJhODkyYmVmZDFkYzI5ZTliOWE3ODIyZWQ5OTg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=X3YHrNeuxedgGj1oFsLBswpF%7EQ%7ExlybYkJVeby8Q1hQYR2KZEQzcpD0-qhIUQnm0c2NcXgBpfLodOvGbs4m8mJSybOV8ZST89LxECGPNsiEXXSjVQOf3pSyEE%7EwrZkWV0Td25yptJTVY8Y7PmopT7nzwKf7dn6vkHXC7pdL2PSt7Fv3jFYm1wjQQ5cUhWwPYi0j%7E2S1KCJjlb-VxF7iL4F92qP09Ld1Cn3fYnn4W-lctKMBs0DSr-pOK3IyNaQmJUj%7EE81hquErLEUETLzUPlOmMwRRUX%7EUBmfIAMpIVBF10tEieBt0drutOvV9EDJH1INEIOCb3tBl43IuXw8vQDg__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.222.139.43, 52.222.139.42, 52.222.139.62, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.222.139.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7323310848 (6.8G) [application/octet-stream]\nSaving to: ‘nous-hermes-13b.ggmlv3.q4_0.bin’\n\nnous-hermes-13b.ggm 100%[===================>]   6.82G  96.1MB/s    in 52s     \n\n2023-07-16 16:48:32 (135 MB/s) - ‘nous-hermes-13b.ggmlv3.q4_0.bin’ saved [7323310848/7323310848]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd models && ls","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:48:32.700815Z","iopub.execute_input":"2023-07-16T16:48:32.702272Z","iopub.status.idle":"2023-07-16T16:48:34.063703Z","shell.execute_reply.started":"2023-07-16T16:48:32.702193Z","shell.execute_reply":"2023-07-16T16:48:34.061698Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"nous-hermes-13b.ggmlv3.q4_0.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain\n!pip install gpt4all\n!pip install chromadb\n!pip install llama-cpp-python\n!pip install urllib3\n!pip install PyMuPDF\n!pip install python-dotenv\n!pip install unstructured\n!pip install extract-msg\n!pip install tabulate\n!pip install pandoc\n!pip install pypandoc\n!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:48:34.067189Z","iopub.execute_input":"2023-07-16T16:48:34.067626Z","iopub.status.idle":"2023-07-16T16:53:32.626192Z","shell.execute_reply.started":"2023-07-16T16:48:34.067585Z","shell.execute_reply":"2023-07-16T16:53:32.624734Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.0.234-py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.5.9)\nCollecting langsmith<0.0.6,>=0.0.5 (from langchain)\n  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.4)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.5)\nCollecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.9)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.6.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\nInstalling collected packages: openapi-schema-pydantic, langsmith, langchain\nSuccessfully installed langchain-0.0.234 langsmith-0.0.5 openapi-schema-pydantic-1.2.4\nCollecting gpt4all\n  Downloading gpt4all-1.0.5-py3-none-manylinux1_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from gpt4all) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gpt4all) (4.65.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->gpt4all) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->gpt4all) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->gpt4all) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->gpt4all) (2023.5.7)\nInstalling collected packages: gpt4all\nSuccessfully installed gpt4all-1.0.5\nCollecting chromadb\n  Downloading chromadb-0.3.29-py3-none-any.whl (396 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.5.3)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic<2.0,>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.10.9)\nCollecting hnswlib>=0.7 (from chromadb)\n  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb)\n  Downloading clickhouse_connect-0.6.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (966 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.7/966.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting duckdb>=0.7.1 (from chromadb)\n  Downloading duckdb-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting fastapi==0.85.1 (from chromadb)\n  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.22.0)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.23.5)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.13.3)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.65.0)\nCollecting overrides>=7.3.1 (from chromadb)\n  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\nCollecting starlette==0.20.4 (from fastapi==0.85.1->chromadb)\n  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb) (3.7.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (6.7.0)\nRequirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.19.0)\nRequirement already satisfied: lz4 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->clickhouse-connect>=0.5.7->chromadb) (3.15.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb) (3.0.9)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.1.1)\nBuilding wheels for collected packages: hnswlib\n  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=186624 sha256=f1d8353eed51d67ca5e39d08b10237267be3aff458a506c5c7b1189178da4af3\n  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\nSuccessfully built hnswlib\nInstalling collected packages: monotonic, duckdb, pulsar-client, overrides, humanfriendly, hnswlib, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.27.0\n    Uninstalling starlette-0.27.0:\n      Successfully uninstalled starlette-0.27.0\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.98.0\n    Uninstalling fastapi-0.98.0:\n      Successfully uninstalled fastapi-0.98.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.3.1 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chromadb-0.3.29 clickhouse-connect-0.6.6 coloredlogs-15.0.1 duckdb-0.8.1 fastapi-0.85.1 hnswlib-0.7.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 starlette-0.20.4\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.1.72.tar.gz (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.72-cp310-cp310-linux_x86_64.whl size=265741 sha256=13ea0d017f965f923d1b0208ae22a9237b83961a4b8aceaccba7395ce0ffc607\n  Stored in directory: /root/.cache/pip/wheels/ed/50/66/f44b417fb8ed7860836157dd767c73d0890c337ead374562c4\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.1 llama-cpp-python-0.1.72\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (1.26.15)\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.22.5\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.0)\nCollecting unstructured\n  Downloading unstructured-0.8.1-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting chardet (from unstructured)\n  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting filetype (from unstructured)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.9.3)\nCollecting msg-parser (from unstructured)\n  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.2.4)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.1.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from unstructured) (1.5.3)\nRequirement already satisfied: pdf2image in /opt/conda/lib/python3.10/site-packages (from unstructured) (1.16.3)\nCollecting pdfminer.six (from unstructured)\n  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from unstructured) (9.5.0)\nCollecting pypandoc (from unstructured)\n  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\nCollecting python-docx (from unstructured)\n  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting python-pptx (from unstructured)\n  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting python-magic (from unstructured)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.4.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from unstructured) (2.31.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured) (0.9.0)\nCollecting xlrd (from unstructured)\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: olefile>=0.46 in /opt/conda/lib/python3.10/site-packages (from msg-parser->unstructured) (0.46)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured) (1.16.0)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->unstructured) (1.1.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->unstructured) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->unstructured) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas->unstructured) (1.23.5)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six->unstructured) (3.1.0)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six->unstructured) (41.0.1)\nCollecting XlsxWriter>=0.5.7 (from python-pptx->unstructured)\n  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (2023.5.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\nBuilding wheels for collected packages: python-docx, python-pptx\n  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=e7b86de53773c876e91007f9ed6db3faf129abeee44cb24aa74d643fd509e110\n  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=a3dc1347928dad652cc8e925927bb22ec522c2c2d776822b9b640caf409de7b6\n  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\nSuccessfully built python-docx python-pptx\nInstalling collected packages: filetype, XlsxWriter, xlrd, python-magic, python-docx, pypandoc, msg-parser, chardet, python-pptx, pdfminer.six, unstructured\nSuccessfully installed XlsxWriter-3.1.2 chardet-5.1.0 filetype-1.2.0 msg-parser-1.2.0 pdfminer.six-20221105 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 unstructured-0.8.1 xlrd-2.0.1\nCollecting extract-msg\n  Downloading extract_msg-0.41.5-py2.py3-none-any.whl (185 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.2/185.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting imapclient<3,>=2.3.0 (from extract-msg)\n  Downloading IMAPClient-2.3.1-py2.py3-none-any.whl (181 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: olefile==0.46 in /opt/conda/lib/python3.10/site-packages (from extract-msg) (0.46)\nRequirement already satisfied: tzlocal<6,>=4.2 in /opt/conda/lib/python3.10/site-packages (from extract-msg) (5.0.1)\nCollecting compressed-rtf<2,>=1.0.6 (from extract-msg)\n  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ebcdic<2,>=1.1.1 (from extract-msg)\n  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: beautifulsoup4<4.13,>=4.11.1 in /opt/conda/lib/python3.10/site-packages (from extract-msg) (4.12.2)\nCollecting RTFDE==0.0.2 (from extract-msg)\n  Downloading RTFDE-0.0.2-py3-none-any.whl (34 kB)\nRequirement already satisfied: chardet<6,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from extract-msg) (5.1.0)\nCollecting red-black-tree-mod==1.20 (from extract-msg)\n  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting lark-parser>=0.11 (from RTFDE==0.0.2->extract-msg)\n  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting oletools>=0.56 (from RTFDE==0.0.2->extract-msg)\n  Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.2/977.2 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<4.13,>=4.11.1->extract-msg) (2.3.2.post1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from imapclient<3,>=2.3.0->extract-msg) (1.16.0)\nCollecting pyparsing<3,>=2.1.0 (from oletools>=0.56->RTFDE==0.0.2->extract-msg)\n  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting easygui (from oletools>=0.56->RTFDE==0.0.2->extract-msg)\n  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting colorclass (from oletools>=0.56->RTFDE==0.0.2->extract-msg)\n  Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\nCollecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE==0.0.2->extract-msg)\n  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\nCollecting msoffcrypto-tool (from oletools>=0.56->RTFDE==0.0.2->extract-msg)\n  Downloading msoffcrypto_tool-5.0.1-py3-none-any.whl (34 kB)\nRequirement already satisfied: cryptography>=35.0 in /opt/conda/lib/python3.10/site-packages (from msoffcrypto-tool->oletools>=0.56->RTFDE==0.0.2->extract-msg) (41.0.1)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE==0.0.2->extract-msg) (1.15.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE==0.0.2->extract-msg) (2.21)\nBuilding wheels for collected packages: red-black-tree-mod, compressed-rtf\n  Building wheel for red-black-tree-mod (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18617 sha256=0dd5914deaecdf0696fb7466c23bc46073d673ebea07bcc3d535b9a3c7ae6131\n  Stored in directory: /root/.cache/pip/wheels/1e/89/a0/17d08e78a59e4e8f51a95fe52e19c6916450c143acc7bce4dd\n  Building wheel for compressed-rtf (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6203 sha256=d59aea5493bc779f87daa8872b7b19d5c5af5a95eb60cd766476ad839414ea59\n  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\nSuccessfully built red-black-tree-mod compressed-rtf\nInstalling collected packages: red-black-tree-mod, lark-parser, ebcdic, easygui, compressed-rtf, pyparsing, imapclient, colorclass, msoffcrypto-tool, pcodedmp, oletools, RTFDE, extract-msg\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.0.9\n    Uninstalling pyparsing-3.0.9:\n      Successfully uninstalled pyparsing-3.0.9\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytoolconfig 1.2.5 requires packaging>=22.0, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed RTFDE-0.0.2 colorclass-2.2.2 compressed-rtf-1.0.6 easygui-0.98.3 ebcdic-1.1.1 extract-msg-0.41.5 imapclient-2.3.1 lark-parser-0.12.0 msoffcrypto-tool-5.0.1 oletools-0.60.1 pcodedmp-1.2.6 pyparsing-2.4.7 red-black-tree-mod-1.20\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (0.9.0)\nCollecting pandoc\n  Downloading pandoc-2.3.tar.gz (33 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting plumbum (from pandoc)\n  Downloading plumbum-1.8.2-py3-none-any.whl (127 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ply (from pandoc)\n  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pandoc\n  Building wheel for pandoc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33284 sha256=32d7feecb49a3f257f912bfa0e3295ea96c1a08608cc504253de38326fc2cc60\n  Stored in directory: /root/.cache/pip/wheels/76/27/c2/c26175310aadcb8741b77657a1bb49c50cc7d4cdbf9eee0005\nSuccessfully built pandoc\nInstalling collected packages: ply, plumbum, pandoc\nSuccessfully installed pandoc-2.3 plumbum-1.8.2 ply-3.11\nRequirement already satisfied: pypandoc in /opt/conda/lib/python3.10/site-packages (1.11)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentence_transformers --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:55:10.545071Z","iopub.execute_input":"2023-07-16T16:55:10.545544Z","iopub.status.idle":"2023-07-16T16:55:27.820709Z","shell.execute_reply.started":"2023-07-16T16:55:10.545508Z","shell.execute_reply":"2023-07-16T16:55:27.819184Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (2.4.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ffcff82f2813fc37c025493f54728f8ee7ce15701ba6facb7538127fcdffc357\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#!/usr/bin/env python3\nimport os\nimport glob\nfrom typing import List\nfrom dotenv import load_dotenv\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom chromadb.config import Settings\n\n\nfrom langchain.document_loaders import (\n    CSVLoader,\n    EverNoteLoader,\n    PyMuPDFLoader,\n    TextLoader,\n    UnstructuredEmailLoader,\n    UnstructuredEPubLoader,\n    UnstructuredHTMLLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredODTLoader,\n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader,\n)\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.docstore.document import Document\n\n\nload_dotenv()\n\n\nPERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\n\npersist_directory = '/kaggle/working'\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=persist_directory,\n        anonymized_telemetry=False\n)\n\n# Load environment variables\nsource_directory = '/kaggle/working/source_documents'\nembeddings_model_name = 'all-mpnet-base-v2'\nchunk_size = 500\nchunk_overlap = 50\n\n\n# Custom document loaders\nclass MyElmLoader(UnstructuredEmailLoader):\n    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n\n    def load(self) -> List[Document]:\n        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n        try:\n            try:\n                doc = UnstructuredEmailLoader.load(self)\n            except ValueError as e:\n                if 'text/html content not found in email' in str(e):\n                    # Try plain text\n                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n                    doc = UnstructuredEmailLoader.load(self)\n                else:\n                    raise\n        except Exception as e:\n            # Add file_path to exception message\n            raise type(e)(f\"{self.file_path}: {e}\") from e\n        return doc\n\n# Map file extensions to document loaders and their arguments\nLOADER_MAPPING = {\n    \".csv\": (CSVLoader, {}),\n    # \".docx\": (Docx2txtLoader, {}),\n    \".doc\": (UnstructuredWordDocumentLoader, {}),\n    \".docx\": (UnstructuredWordDocumentLoader, {}),\n    \".enex\": (EverNoteLoader, {}),\n    \".eml\": (MyElmLoader, {}),\n    \".epub\": (UnstructuredEPubLoader, {}),\n    \".html\": (UnstructuredHTMLLoader, {}),\n    \".md\": (UnstructuredMarkdownLoader, {}),\n    \".odt\": (UnstructuredODTLoader, {}),\n    \".pdf\": (PyMuPDFLoader, {}),\n    \".ppt\": (UnstructuredPowerPointLoader, {}),\n    \".pptx\": (UnstructuredPowerPointLoader, {}),\n    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n    # Add more mappings for other file extensions and loaders as needed\n}\n\n\ndef load_single_document(file_path: str) -> List[Document]:\n    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n    if ext in LOADER_MAPPING:\n        loader_class, loader_args = LOADER_MAPPING[ext]\n        loader = loader_class(file_path, **loader_args)\n        return loader.load()\n    raise ValueError(f\"Unsupported file extension '{ext}'\")\n\ndef load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Loads all documents from the source documents directory, ignoring specified files\n    \"\"\"\n    all_files = []\n    for ext in LOADER_MAPPING:\n        all_files.extend(\n            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n        )\n    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n    with Pool(processes=os.cpu_count()) as pool:\n        results = []\n        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n                results.extend(docs)\n                pbar.update()\n    return results\n\ndef process_documents(ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Load documents and split in chunks\n    \"\"\"\n    print(f\"Loading documents from {source_directory}\")\n    documents = load_documents(source_directory, ignored_files)\n    if not documents:\n        print(\"No new documents to load\")\n        exit(0)\n    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    texts = text_splitter.split_documents(documents)\n    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n    return texts\n\ndef does_vectorstore_exist(persist_directory: str) -> bool:\n    \"\"\"\n    Checks if vectorstore exists\n    \"\"\"\n    if os.path.exists(os.path.join(persist_directory, 'index')):\n        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n            # At least 3 documents are needed in a working vectorstore\n            if len(list_index_files) > 3:\n                return True\n    return False\n\ndef main():\n    # Create embeddings\n    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n    if does_vectorstore_exist(persist_directory):\n        # Update and store locally vectorstore\n        print(f\"Appending to existing vectorstore at {persist_directory}\")\n        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n        collection = db.get()\n        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n        print(f\"Creating embeddings. May take some minutes...\")\n        db.add_documents(texts)\n    else:\n        # Create and store locally vectorstore\n        print(\"Creating new vectorstore\")\n        texts = process_documents()\n        print(f\"Creating embeddings. May take some minutes...\")\n        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n    db.persist()\n    db = None\n    print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:55:40.603840Z","iopub.execute_input":"2023-07-16T16:55:40.604263Z","iopub.status.idle":"2023-07-16T16:56:57.293639Z","shell.execute_reply.started":"2023-07-16T16:55:40.604226Z","shell.execute_reply":"2023-07-16T16:56:57.292243Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4fb7f4da5841fbb910bb2ed6a0a945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a052f0a1fd45829823775a9f769497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1261e5905c1e4108b8bff2bc38da8dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28811e52f46a473b9db4c2c14d8d7107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e56453619c44d5957428922c855277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"732a8b25361e466fb6842bd5ee563687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918f04c9090b48809e81e2240873fbd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9026b25476b54843903cfc6d26499fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b210660dd0244ecea27bd2a2a75cc6d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f8d5a1524e2435d9a5c9e1080fc8ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b13740500d2548fba7c2b4df9dd07f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe11db001fb433ababbe7ae3699e9b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5976dd9cfb9549f1af5ac3c985706e89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"534cd94df30241cd8686353e0cf413cb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Creating new vectorstore\nLoading documents from /kaggle/working/source_documents\n","output_type":"stream"},{"name":"stderr","text":"Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00, 51.37it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded 1 new documents from /kaggle/working/source_documents\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Split into 107 chunks of text (max. 500 tokens each)\nCreating embeddings. May take some minutes...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56df7879f4b4a0e84b5942799e7ed2b"}},"metadata":{}},{"name":"stdout","text":"Ingestion complete! You can now run privateGPT.py to query your documents\n","output_type":"stream"}]},{"cell_type":"code","source":"#!/usr/bin/env python3\nfrom dotenv import load_dotenv\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import GPT4All, LlamaCpp\nimport os\nimport argparse\nimport time\nfrom chromadb.config import Settings\n\nload_dotenv()\n\npersist_directory = '/kaggle/working'\nsource_directory = '/kaggle/working/source_documents'\nembeddings_model_name = 'all-mpnet-base-v2'\nmodel_path='/kaggle/working/models/nous-hermes-13b.ggmlv3.q4_0.bin'\nmodel_n_ctx=1000\ntarget_source_chunks=4\nmodel_n_batch=8\nmodel_type='LlamaCpp'\n#model_type='GPT4All'\nmute_stream = True\nhide_source = False\n#model_type = os.environ.get('MODEL_TYPE')\n#model_path = os.environ.get('MODEL_PATH')\n#model_n_ctx = os.environ.get('MODEL_N_CTX')\n#model_n_batch = int(os.environ.get('MODEL_N_BATCH',8))\n#target_source_chunks = int(os.environ.get('TARGET_SOURCE_CHUNKS',4))\n\ndef main():\n    # Parse the command line arguments\n    #args = parse_arguments()\n    # Define the Chroma settings\n    CHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=persist_directory,\n        anonymized_telemetry=False\n    )\n    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n    retriever = db.as_retriever(search_kwargs={\"k\": target_source_chunks})\n    # activate/deactivate the streaming StdOut callback for LLMs\n    callbacks = [] if mute_stream else [StreamingStdOutCallbackHandler()]\n    # Prepare the LLM\n    match model_type:\n        case \"LlamaCpp\":\n            #llm = Model(model_path=model_path,n_ctx=model_n_ctx,)\n            llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, n_batch=model_n_batch, callbacks=callbacks, verbose=False)\n        case \"GPT4All\":\n            llm = GPT4All(model=model_path, n_ctx=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False)\n        case _default:\n            # raise exception if model_type is not supported\n            raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All\")\n        \n    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents= not hide_source)\n    # Interactive questions and answers\n    while True:\n        query = input(\"\\nEnter a query: \")\n        if query == \"exit\":\n            break\n        if query.strip() == \"\":\n            continue\n        # Get the answer from the chain\n        start = time.time()\n        res = qa(query)\n        answer, docs = res['result'], [] if hide_source else res['source_documents']\n        end = time.time()\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n        print(answer)\n        # Print the relevant sources used for the answer\n        for document in docs:\n            print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n            print(document.page_content)\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '\n                                                 'using the power of LLMs.')\n    parser.add_argument(\"--hide-source\", \"-S\", action='store_true',\n                        help='Use this flag to disable printing of source documents used for answers.')\n    parser.add_argument(\"--mute-stream\", \"-M\",\n                        action='store_true',\n                        help='Use this flag to disable the streaming StdOut callback for LLMs.')\n    return parser.parse_args()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T16:58:11.232679Z","iopub.execute_input":"2023-07-16T16:58:11.233143Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"llama.cpp: loading model from /kaggle/working/models/nous-hermes-13b.ggmlv3.q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32001\nllama_model_load_internal: n_ctx      = 1000\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =    0.09 MB\nllama_model_load_internal: mem required  = 8861.72 MB (+ 1608.00 MB per state)\nllama_new_context_with_model: kv self size  =  781.25 MB\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query:  What is this extract all about ?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba4302975a840198adf022ee842db7d"}},"metadata":{}},{"name":"stdout","text":"\n\n> Question:\nWhat is this extract all about ?\n\n> Answer (took 441.28 s.):\n This extract is from the state of the union speech by the President of the United States. It talks about several things such as Ohio State football games, building Legos with their daughter, cancer from prolonged exposure to burn pits, Heath's fight against cancer, Danielle's purpose to demand better, and the VA's efforts to improve care for veterans.\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\n\",\"\",\"Heath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \",\"\",\"But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \",\"\",\"Danielle says Heath was a fighter to the very end. \",\"\",\"He didn’t know how to stop fighting, and neither did she. \",\"\",\"Through her pain she found purpose to demand we do better. \",\"\",\"Tonight, Danielle—we are. \",\"\",\"The VA is pioneering new ways of linking\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\name\":\"state_of_the_union.txt\",\"displayUrl\":\"https://github.com/imartinez/privateGPT/blob/main/source_documents/state_of_the_union.txt?raw=true\",\"headerInfo\":{\"blobSize\":\"38.1\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\nat a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  \",\"\",\"If you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \",\"\",\"We’re leaving no one behind or ignoring anyone’s needs as we move forward. \",\"\",\"And on testing, we have made hundreds of millions of tests available for you to order for free.   \",\"\",\"Even if you already ordered free tests tonight, I am announcing that you can order more from\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\n\"isOrgOwned\":false},\"refInfo\":{\"name\":\"main\",\"listCacheKey\":\"v0:1684445009.429842\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"b1057afdf8f65fdb10e4160adbd8462be0c08271\"},\"path\":\"source_documents/state_of_the_union.txt\",\"currentUser\":null,\"blob\":{\"rawLines\":[\"Madam\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query:  Who is the main speaker of this extract ?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee5307c5cdb427fb3a8a8a052d4a26e"}},"metadata":{}},{"name":"stdout","text":"\n\n> Question:\nWho is the main speaker of this extract ?\n\n> Answer (took 427.37 s.):\n The main speaker of this extract is not explicitly stated. However, based on the context given, it appears that the speaker is the President or another high-ranking official giving a speech at an event such as the State of the Union address or a presidential debate.\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\nhe will never gain the hearts and souls of the Ukrainian people. \",\"\",\"He will never extinguish their love of freedom. He will never weaken the resolve of the free world. \",\"\",\"We meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \",\"\",\"The pandemic has been punishing. \",\"\",\"And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \",\"\",\"I understand. \",\"\",\"I\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\n\",\"\",\"Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \",\"\",\"Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \",\"\",\"We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \",\"\",\"I recently visited the New York City Police Department days after the funerals of Officer\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\nSpeaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \",\"\",\"Last year COVID-19 kept us apart. This year we are finally together again. \",\"\",\"Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \",\"\",\"With a duty to one another to the American people to the Constitution. \",\"\",\"And with an unwavering resolve that freedom will always triumph over\n\n> /kaggle/working/source_documents/state_of_the_union.txt:\n\",\"\",\"The most fundamental right in America is the right to vote – and to have it counted. And it’s under assault. \",\"\",\"In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \",\"\",\"We cannot let this happen. \",\"\",\"Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \",\"\",\"Tonight, I’d like\n","output_type":"stream"}]}]}